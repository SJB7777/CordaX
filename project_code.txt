===== .\analysis_main.py =====
import os
from typing import TYPE_CHECKING, Optional

import numpy as np
import numpy.typing as npt
import tifffile
from roi_rectangle import RoiRectangle

from src.gui.roi import RoiSelector
from src.utils.file_util import create_run_scan_directory
from src.config.config import load_config
from src.logger import setup_logger, Logger
from src.analyzer.draw_figure import (
    patch_rectangle,
    draw_com_figure,
    draw_intensity_figure,
    draw_intensity_diff_figure,
    draw_com_diff_figure
)
from src.analyzer.core import DataAnalyzer

if TYPE_CHECKING:
    from pandas import DataFrame
    from matplotlib.figure import Figure
    from src.config.config import ExpConfig


def main() -> None:
    config: ExpConfig = load_config()
    logger: Logger = setup_logger()

    # Define run and scan numbers
    run_num: int = 1
    scan_num: int = 1
    comment: Optional[str] = None

    # Define file paths and names
    npz_dir: str = config.path.npz_dir
    file_name: str = f"run={run_num:0>4}_scan={scan_num:0>4}"
    if comment is not None:
        file_name += comment
    npz_file: str = os.path.join(npz_dir, file_name + ".npz")

    if not os.path.exists(npz_file):
        logger.error(f"The file {npz_file} does not exist.")
        raise FileNotFoundError(f"The file {npz_file} does not exist.")

    logger.info(f"Run DataAnalyzer run={run_num:0>3} scan={scan_num:0>3}")
    # Initialize MeanDataProcessor
    processor: DataAnalyzer = DataAnalyzer(npz_file, 0)

    # Extract images
    poff_images: npt.NDArray = processor.poff_images
    pon_images: npt.NDArray = processor.pon_images

    # Select ROI using GUI
    roi: Optional[tuple[int, int, int, int]] = RoiSelector().select_roi(
        np.log1p(poff_images.sum(axis=0))
    )
    if roi is None:
        logger.error(f"No ROI Rectangle Set for run={run_num}, scan={scan_num}")
        raise ValueError(f"No ROI Rectangle Set for run={run_num}, scan={scan_num}")

    logger.info(f"ROI rectangle: {roi}")
    roi_rect: RoiRectangle = RoiRectangle.from_tuple(roi)

    # Analyze data within the selected ROI
    data_df: DataFrame = processor.analyze_by_roi(roi_rect)

    # Define save directory
    image_dir: str = config.path.image_dir
    anaylsis_dir: str = create_run_scan_directory(image_dir, run_num, scan_num)

    # Slice images to ROI
    roi_poff_images: npt.NDArray = roi_rect.slice(poff_images)
    roi_pon_images: npt.NDArray = roi_rect.slice(pon_images)

    # Save images as TIFF files
    tifffile.imwrite(os.path.join(anaylsis_dir, "poff.tif"), poff_images.astype(np.float32))
    logger.info(f"Saved TIF '{os.path.join(anaylsis_dir, 'poff.tif')}'")

    tifffile.imwrite(os.path.join(anaylsis_dir, 'pon.tif'), pon_images.astype(np.float32))
    logger.info(f"Saved TIF '{os.path.join(anaylsis_dir, 'pon.tif')}'")

    tifffile.imwrite(os.path.join(anaylsis_dir, "roi_poff.tif"), roi_poff_images.astype(np.float32))
    logger.info(f"Saved TIF '{os.path.join(anaylsis_dir, 'roi_poff.tif')}'")

    tifffile.imwrite(os.path.join(anaylsis_dir, "roi_pon.tif"), roi_pon_images.astype(np.float32))
    logger.info(f"Saved TIF '{os.path.join(anaylsis_dir, 'roi_pon.tif')}'")

    # Save data as CSV
    data_file: str = os.path.join(anaylsis_dir, "data.csv")
    data_df.to_csv(data_file)
    logger.info(f"Saved CSV '{data_file}'")

    # Create figures
    image_fig: Figure = patch_rectangle(
        np.log1p(processor.poff_images.sum(axis=0)),
        *roi_rect.to_tuple()
    )
    intensity_fig: Figure = draw_intensity_figure(data_df)
    intensity_diff_fig: Figure = draw_intensity_diff_figure(data_df)
    com_fig: Figure = draw_com_figure(data_df)
    com_diff_fig: Figure = draw_com_diff_figure(data_df)

    # Save figures as PNG files
    image_fig.savefig(os.path.join(anaylsis_dir, "log_image.png"))
    logger.info(f"Saved PNG '{os.path.join(anaylsis_dir, 'log_image.png')}'")

    intensity_fig.savefig(os.path.join(anaylsis_dir, "delay-intensity.png"))
    logger.info(f"Saved PNG '{os.path.join(anaylsis_dir, 'delay-intensity.png')}'")

    intensity_diff_fig.savefig(os.path.join(anaylsis_dir, "delay-intensity_diff.png"))
    logger.info(f"Saved PNG '{os.path.join(anaylsis_dir, 'delay-intensity_diff.png')}'")

    com_fig.savefig(os.path.join(anaylsis_dir, "delay-com.png"))
    logger.info(f"Saved PNG '{os.path.join(anaylsis_dir, 'delay-com.png')}'")

    com_diff_fig.savefig(os.path.join(anaylsis_dir, "delay-com_diff.png"))
    logger.info(f"Saved PNG '{os.path.join(anaylsis_dir, 'delay-com_diff.png')}'")

    logger.info(f"Run DataAnalyzer run={run_num:0>3} scan={scan_num:0>3} is Done.")


if __name__ == "__main__":
    main()


===== .\processing_main.py =====
import os
from typing import Optional

from roi_rectangle import RoiRectangle

from src.logger import setup_logger, Logger
from src.processor.core import CoreProcessor
from src.processor.loader import HDF5FileLoader
from src.processor.saver import SaverFactory, SaverStrategy
from src.preprocessor.image_qbpm_preprocessor import (
    compose,
    subtract_dark_background,
    normalize_images_by_qbpm,
    create_ransac_roi_outlier_remover,
    ImagesQbpmProcessor
)
from src.gui.roi import get_roi_auto, get_hdf5_images
from src.utils.file_util import get_folder_list, get_run_scan_directory, get_file_list
from src.config.config import load_config, ExpConfig


logger: Logger = setup_logger()


def get_scan_nums(run_num: int, config: ExpConfig) -> list[int]:
    """Get Scan numbers from real directory"""
    run_dir: str = get_run_scan_directory(config.path.load_dir, run_num)
    scan_folders: list[str] = get_folder_list(run_dir)
    return [int(scan_dir.split("=")[1]) for scan_dir in scan_folders]


def get_roi(scan_dir: str, config: ExpConfig, index_mode: Optional[int] = None) -> RoiRectangle:
    """Get Roi for QBPM Normalization"""
    files = get_file_list(scan_dir)

    if index_mode is None:
        index = len(files) // 2
    else:
        index = index_mode

    file: str = os.path.join(scan_dir, files[index])
    image = get_hdf5_images(file, config).sum(axis=0)
    return get_roi_auto(image)


def setup_preprocessors(roi_rect: RoiRectangle) -> dict[str, ImagesQbpmProcessor]:
    """Return preprocessors"""

    remove_by_ransac_roi: ImagesQbpmProcessor = create_ransac_roi_outlier_remover(roi_rect)

    standard_preprocessor = compose(
        subtract_dark_background,
        remove_by_ransac_roi,
        normalize_images_by_qbpm,
    )

    return {
        "standard": standard_preprocessor,
    }


def process_scan(run_num: int, scan_num: int, config: ExpConfig) -> None:
    """Process Single Scan"""

    load_dir = config.path.load_dir
    scan_dir = get_run_scan_directory(load_dir, run_num, scan_num)

    roi_rect = get_roi(scan_dir, config)
    if roi_rect is None:
        raise ValueError(f"No ROI Rectangle Set for run={run_num}, scan={scan_num}")
    logger.info(f"ROI rectangle: {roi_rect.to_tuple()}")
    preprocessors: dict[str, ImagesQbpmProcessor] = setup_preprocessors(roi_rect)

    for preprocessor_name in preprocessors:
        logger.info(f"preprocessor: {preprocessor_name}")

    processor: CoreProcessor = CoreProcessor(HDF5FileLoader, preprocessors, logger)
    processor.scan(scan_dir)

    file_name: str = f"run={run_num:0>4}_scan={scan_num:0>4}"
    # mat_saver: SaverStrategy = SaverFactory.get_saver("mat")
    npz_saver: SaverStrategy = SaverFactory.get_saver("npz")
    processor.save(npz_saver, file_name)

    logger.info(f"Processing run={run_num}, scan={scan_num} is complete")


def main() -> None:
    """
    60 Hz laser:
    197, 201, 202, 203, 204, 212, 213, 214, 217, 218,
    219, 220, 221, 222, 223, 228, 229, 230, 231, 234,
    235, 236, 237, 238, 241, 242, 243, 244, 246, 251,
    252, 253, 254, 255, 256, 259, 260, 261, 262, 263
    """

    config = load_config()
    run_nums: list[int] = config.runs
    logger.info(f"Runs to process: {run_nums}")

    for run_num in run_nums:
        logger.info(f"Run: {run_num}")
        scan_nums: list[int] = get_scan_nums(run_num, config)
        for scan_num in scan_nums:
            try:
                process_scan(run_num, scan_num, config)
            except Exception:
                logger.exception(f"Failed to process run={run_num}, scan={scan_num}")
                raise

    logger.info("All processing is complete")


if __name__ == "__main__":
    main()


===== .\src\logger.py =====
"""
Module for setting up a customized logger using the loguru library.

This module provides a function to configure and return a logger instance
with specific formatting, log file settings, and rotation/compression options.

Log files are stored in the 'logs' directory, organized by date, and named with a timestamp.
Each log file is rotated when it reaches 500 MB in size and compressed in ZIP format.

Example usage:
    from logger_setup import setup_logger
    logger = setup_logger()
    logger.info("This is an info message.")
"""
import loguru
from loguru._logger import Logger


def setup_logger() -> Logger:
    """
    Configures and sets up the logger with a custom format and log file settings.

    Returns:
        Logger: The configured logger instance.
    """
    formatter = "{time:YYYY-MM-DD HH:mm:ss.SSS} | {level} | {name}:{function}:{line} - {message}"
    log_file: str = "logs\\{time:YYYY-MM-DD}\\{time:YYYYMMDD_HHmmss}.log"
    loguru.logger.add(log_file, format=formatter, rotation="500 MB", compression="zip")

    return loguru.logger


if __name__ == "__main__":
    logger = setup_logger()
    logger.debug("This is a debug message.")

    logger.info("This is an info message.")

    logger.warning("This is a warning message.")

    logger.error("This is an error message.")

    try:
        raise ValueError("This is a test exception.")
    except ValueError as e:
        logger.exception(f"An exception occurred: {e}")

    metadata = {"key1": "value1", "key2": "value2"}
    logger.info(metadata)

    print("All tests passed.")


===== .\src\analyzer\core.py =====
import os
from typing import Mapping

import numpy as np
import numpy.typing as npt
import pandas as pd
from scipy.ndimage import rotate
from scipy.optimize import curve_fit
from roi_rectangle import RoiRectangle

from src.utils.math_util import gaussian, mul_delta_q


class DataAnalyzer:
    def __init__(self, file: str, angle: int = 0) -> None:
        if not os.path.exists(file):
            raise FileNotFoundError(f"The file {file} does not exist.")

        data: Mapping[str, npt.NDArray] = np.load(file)

        if "delay" not in data or "pon" not in data or "poff" not in data:
            raise ValueError(
                "The file does not contain the required keys: 'delay', 'pon', 'poff'"
            )

        self.delay: npt.NDArray = data["delay"]
        self.poff_images: npt.NDArray = data["poff"]
        self.pon_images: npt.NDArray = data["pon"]

        if angle:
            self.poff_images = rotate(self.poff_images, 45, axes=(1, 2), reshape=False)
            self.pon_images = rotate(self.pon_images, 45, axes=(1, 2), reshape=False)

    def get_summed_image(self) -> tuple[npt.NDArray, npt.NDArray]:
        """
        return:
            pump off image, pump on image
        """
        return self.poff_images.sum(axis=0), self.pon_images.sum(axis=0)

    def pon_subtract_by_poff(self):
        return np.maximum(self.pon_images - self.poff_images, 0)

    def _roi_center_of_masses(
        self,
        roi_rect: RoiRectangle,
        images: npt.NDArray
    ) -> tuple[npt.NDArray, npt.NDArray]:
        roi_images = roi_rect.slice(images)
        height, width = roi_rect.height, roi_rect.width

        y_coords, x_coords = np.mgrid[:height, :width]

        total_mass = np.sum(roi_images, axis=(1, 2))
        x_centroids = np.sum(x_coords * roi_images, axis=(1, 2)) / total_mass
        y_centroids = np.sum(y_coords * roi_images, axis=(1, 2)) / total_mass

        return x_centroids, y_centroids

    def _roi_gaussian(
        self,
        roi_rect: RoiRectangle,
        images: npt.NDArray
    ) -> tuple[npt.NDArray, npt.NDArray, npt.NDArray]:

        roi_images = roi_rect.slice(images)
        height, width = roi_rect.height, roi_rect.width

        intensities = []
        com_xs = []
        com_ys = []

        for image in roi_images:
            max_y, max_x = np.unravel_index(np.argmax(image), image.shape)
            # max_y, max_x = height // 2, width // 2

            x: npt.NDArray = np.arange(0, width)
            y: npt.NDArray = np.arange(0, height)

            x_data = image.sum(axis=0)
            y_data = image.sum(axis=1)

            initial_guess_x = [x_data[max_x], max_x, (np.max(x_data) - np.min(x_data)) / 4]
            try:
                params_x = curve_fit(gaussian, x, x_data, p0=initial_guess_x)[0]
            except RuntimeError as e:
                print(e, ": x")
                params_x = [np.nan, np.nan, np.nan]

            initial_guess_y = [y_data[max_y], max_y, (np.max(y_data) - np.min(y_data)) / 4]
            try:
                params_y = curve_fit(gaussian, y, y_data, p0=initial_guess_y)[0]
            except RuntimeError as e:
                print(e, ": y")
                params_y = [np.nan, np.nan, np.nan]

            gaussian_a_x, gaussain_com_x, _ = params_x
            gaussian_a_y, gaussain_com_y, _ = params_y

            gaussian_a = np.sqrt(gaussian_a_x * gaussian_a_y)
            intensities.append(gaussian_a)
            com_xs.append(gaussain_com_x)
            com_ys.append(gaussain_com_y)

        return np.stack(intensities), np.stack(com_xs), np.stack(com_ys)

    def _roi_intensities(self, roi_rect: RoiRectangle, images: npt.NDArray):
        roi_images = roi_rect.slice(images)

        return roi_images.mean(axis=(1, 2))

    def analyze_by_roi(self, roi_rect: RoiRectangle) -> pd.DataFrame:

        poff_com_x, poff_com_y = self._roi_center_of_masses(roi_rect, self.poff_images)
        poff_intensity = self._roi_intensities(roi_rect, self.poff_images)
        pon_com_x, pon_com_y = self._roi_center_of_masses(roi_rect, self.pon_images)
        pon_intensity = self._roi_intensities(roi_rect, self.pon_images)

        # poff_guassain_intensity, poff_gussian_com_x, poff_gussian_com_y = self._roi_gaussian(roi_rect, self.poff_images)
        # pon_guassain_intensity, pon_gussian_com_x, pon_gussian_com_y = self._roi_gaussian(roi_rect, self.pon_images)

        roi_df = pd.DataFrame(data={
            "poff_com_x": mul_delta_q(poff_com_x - poff_com_x[0]),
            "poff_com_y": mul_delta_q(poff_com_y - poff_com_y[0]),
            "poff_intensity": poff_intensity / poff_intensity[0],
            "pon_com_x": mul_delta_q(pon_com_x - pon_com_x[0]),
            "pon_com_y": mul_delta_q(pon_com_y - pon_com_y[0]),
            "pon_intensity": pon_intensity / pon_intensity[0],

            # "poff_gussian_com_x": poff_gussian_com_x,
            # "poff_gussian_com_y": poff_gussian_com_y,
            # "poff_guassain_intensity": poff_guassain_intensity / poff_guassain_intensity[0],

            # "pon_gussian_com_x": pon_gussian_com_x,
            # "pon_gussian_com_y": pon_gussian_com_y,
            # "pon_guassain_intensity": pon_guassain_intensity / pon_guassain_intensity[0],

        })

        roi_df = roi_df.set_index(self.delay)
        return roi_df


if __name__ == "__main__":

    print("Run analyzer.core")


===== .\src\analyzer\draw_figure.py =====
from typing import TYPE_CHECKING

import numpy as np
import numpy.typing as npt
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches

if TYPE_CHECKING:
    from matplotlib.figure import Figure


def patch_rectangle(
    image: npt.NDArray,
    x1: int, y1: int,
    x2: int, y2: int
) -> 'Figure':
    patched_image = np.copy(image)

    fig, ax = plt.subplots(figsize=(10, 10))

    ax.imshow(patched_image)

    rect = patches.Rectangle(
        (x1, y1), x2 - x1, y2 - y1,
        linewidth=1, edgecolor='r', facecolor='none'
    )

    ax.add_patch(rect)

    ax.set_title('Patched Image')
    ax.set_xlabel('X-axis')
    ax.set_ylabel('Y-axis')

    return fig


def draw_intensity_figure(data_df: pd.DataFrame) -> 'Figure':
    delay = data_df.index
    poff_intensity = data_df["poff_intensity"]
    pon_intensity = data_df["pon_intensity"]

    fig, ax = plt.subplots()
    ax.plot(delay, poff_intensity, label="poff_intensity", marker='o')
    ax.plot(delay, pon_intensity, label="pon_intensity", marker='x')

    ax.set_xlabel("Delay [ps]")
    ax.set_ylabel("Intensity [a.u.]")
    ax.set_title("Intensity vs Delay")
    ax.legend()

    plt.tight_layout()

    return fig


def draw_com_figure(data_df: pd.DataFrame) -> 'Figure':
    delay = data_df.index
    poff_com_x = data_df["poff_com_x"]
    pon_com_x = data_df["pon_com_x"]
    poff_com_y = data_df["poff_com_y"]
    pon_com_y = data_df["pon_com_y"]

    fig, axs = plt.subplots(2, 1, figsize=(10, 8), sharex=True)

    # Plot poff_com_x and pon_com_x
    axs[0].plot(delay, poff_com_x, label='poff_com_x', marker='o', color='b')
    axs[0].plot(delay, pon_com_x, label='pon_com_x', marker='x', color='r')
    axs[0].set_title('COM X Position')
    axs[0].set_ylabel('Position X($Q_z$) [Å$^{-1}$]')
    axs[0].legend()

    # Plot poff_com_y and pon_com_y
    axs[1].plot(delay, poff_com_y, label='poff_com_y', marker='o', color='g')
    axs[1].plot(delay, pon_com_y, label='pon_com_y', marker='x', color='y')
    axs[1].set_title('COM Y Position')
    axs[1].set_xlabel('Delay [ps]')
    axs[1].set_ylabel('Position Y(2$\\theta$) [Å$^{-1}$]')
    axs[1].legend()

    plt.tight_layout()

    return fig


def draw_intensity_diff_figure(data_df: pd.DataFrame) -> 'Figure':
    delay = data_df.index
    poff_intensity = data_df["poff_intensity"]
    pon_intensity = data_df["pon_intensity"]

    # Calculate the difference between pon_intensity and poff_intensity
    intensity_difference = pon_intensity - poff_intensity

    fig, ax = plt.subplots()
    ax.plot(delay, intensity_difference, label="Intensity Difference (poff - pon)", marker='o')

    ax.set_xlabel("Delay [ps]")
    ax.set_ylabel("Intensity Difference [a.u.]")
    ax.set_title("Intensity Difference (poff - pon) vs Delay")
    ax.legend()

    plt.tight_layout()

    return fig


def draw_com_diff_figure(data_df: pd.DataFrame) -> 'Figure':
    delay = data_df.index
    poff_com_x = data_df["poff_com_x"]
    pon_com_x = data_df["pon_com_x"]
    poff_com_y = data_df["poff_com_y"]
    pon_com_y = data_df["pon_com_y"]

    # Calculate the difference between pon_com_x and poff_com_x
    com_x_difference = poff_com_x - pon_com_x

    # Calculate the difference between pon_com_y and poff_com_y
    com_y_difference = poff_com_y - pon_com_y

    fig, axs = plt.subplots(2, 1, figsize=(10, 8), sharex=True)

    # Plot com_x_difference
    axs[0].plot(delay, com_x_difference, label='COM X Difference (poff - pon)', marker='o', color='b')
    axs[0].set_title('COM X Position Difference (poff - pon)')
    axs[0].set_ylabel('Position X Difference($Q_z$) [Å$^{-1}$]')
    axs[0].legend()

    # Plot com_y_difference
    axs[1].plot(delay, com_y_difference, label='COM Y Difference (poff - pon)', marker='o', color='g')
    axs[1].set_title('COM Y Position Difference (poff - pon)')
    axs[1].set_xlabel('Delay [ps]')
    axs[1].set_ylabel('Position Y Difference(2$\\theta$) [Å$^{-1}$]')
    axs[1].legend()

    plt.tight_layout()

    return fig


if __name__ == "__main__":
    print("run analyzer.draw_figure")


===== .\src\analyzer\loader.py =====
from scipy.io import loadmat
import numpy.typing as npt


class MatLoader:
    def __init__(self, file):
        mat_images: npt.NDArray = loadmat(file)["data"]
        images = mat_images.swapaxes(0, 2)
        self.images = images.swapaxes(1, 2)


if __name__ == "__main__":
    import os
    from src.config.config import load_config
    config = load_config()
    mat_dir = config.path.mat_dir
    file_name = "run=0143_scan=0001_poff"
    mat_file = os.path.join(mat_dir, file_name + ".mat")
    mat_loader = MatLoader(mat_file)
    off_images = mat_loader.images

    file_name = "run=0143_scan=0001_pon"
    mat_file = os.path.join(mat_dir, file_name + ".mat")
    mat_loader = MatLoader(mat_file)
    on_images = mat_loader.images

    print(f"{off_images.shape = }")
    print(f"{on_images.shape = }")


===== .\src\analyzer\roi_tracker.py =====
from typing import Mapping

import numpy as np
import numpy.typing as npt
from roi_rectangle import RoiRectangle
from matplotlib import patches

from src.gui.roi import RoiSelector
from src.config.config import load_config, ExpConfig


if __name__ == "__main__":
    import os

    import matplotlib.pyplot as plt

    run = 154
    scan = 1

    config: ExpConfig = load_config()
    npz_dir: str = config.path.npz_dir
    npz_file = os.path.join(npz_dir, f"run={run:0>4}_scan={scan:0>4}.npz")
    data: Mapping[str, npt.NDArray] = np.load(npz_file)
    delays: npt.NDArray = data["delay"]
    images: npt.NDArray = data["pon"]
    
    init_roi = RoiSelector().select_roi(np.log1p(images[0]))
    init_roi_rect: RoiRectangle = RoiRectangle.from_tuple(init_roi)
    roi_rects: list[RoiRectangle] = [init_roi_rect]

    for image in images:
        roi_image = roi_rects[-1].slice(image)
        com = np.unravel_index(np.argmax(roi_image), roi_image.shape)[::-1]
        new_center = (
            round(roi_rects[-1].x1 + com[0]),
            round(roi_rects[-1].y1 + com[1])
        )
        new_roi_rect = init_roi_rect.move_to_center(new_center)
        roi_rects.append(new_roi_rect)

    xs = []
    ys = []
    for roi_rect, image in zip(roi_rects[1:], images):
        roi_rect.slice(image)
        com = np.unravel_index(np.argmax(roi_image), roi_image.shape)[::-1]
        x = roi_rect.x1 + com[0]
        y = roi_rect.y1 + com[1]

        xs.append(x)
        ys.append(y)

    # fig, axs = plt.subplots(2, 1)
    # axs[0].plot(delays, xs)
    # axs[1].plot(delays, ys)

    # plt.show()

    for idx, image in enumerate(np.log1p(images)):

        # 이미지와 ROI를 그릴 그림 생성
        fig, ax = plt.subplots()
        ax.imshow(image)

        # 해당 이미지의 ROI 정보 가져오기
        roi_rect = roi_rects[idx]
        rect = patches.Rectangle((roi_rect.x1, roi_rect.y1), roi_rect.width, roi_rect.height, linewidth=1, edgecolor='r', facecolor='none')
        ax.add_patch(rect)

        # 이미지와 ROI를 파일로 저장
        output_path = os.path.join("Y:\\240608_FXS\\raw_data\\h5\\type=raw\\Image\\moving_roi", f"image_{idx}.png")
        plt.savefig(output_path)
        plt.close(fig)


===== .\src\config\config.py =====
import yaml

from src.config.config_definitions import ExpConfig

def load_config() -> ExpConfig:
    """load config file and return config object"""
    with open("config.yaml", 'r', encoding="utf-8") as f:
        config_dict = yaml.safe_load(f)
    return ExpConfig(**config_dict)


def save_config(config_dict: dict) -> None:
    """get config dict and save to file"""
    with open("config.yaml", 'w', encoding="utf-8") as f:
        yaml.safe_dump(config_dict, f, default_flow_style=False, sort_keys=False)


if __name__ == "__main__":
    from src.config.enums import Hutch, Detector, Xray, Hertz


    # config = load_config()
    # print(config.runs)
    config_dict = {
        "runs": [1, 2, 3],
        "path": {
            # Mother Directory of run files.
            "load_dir": "Y:\\240608_FXS\\raw_data\\h5\\type=raw",
            "anaylsis_dir": "Y:\\240608_FXS\\raw_data\\h5\\type=raw",
            # "load_dir": "D:\\dev\\p_python\\xrd\\xfel_sample_data",
            # "anaylsis_dir": "D:\\dev\\p_python\\xrd\\xfel_sample_data",
            # relative path based on anaylsis_dir
            "image_dir": "Image",
            "param_dir": "DataParameter",
            "mat_dir": "Mat_files2",
            "npz_dir": "Npz_files",
            "tif_dir": "Tif_files"
        },
        "param": {
            # Hutch
            "hutch": Hutch.EH1.value,
            # Detector
            "detector": Detector.JUNGFRAU2.value,
            # Xray used in experiment.
            "xray": Xray.HARD.value,
            # Rate of laser.
            "pump_setting": Hertz.FIFTEEN.value,
            # Index of roi coordinate inside h5 file.
            "x1": 0, "x2": 1, "y1": 2, "y2": 3,
            # Metric of SDD and DPS is meters.
            "sdd": 1.3,
            "dps": 7.5e-5,  # Detector Pixel Size
            "beam_energy": 9.7,
        }
    }

    save_config(config_dict)


===== .\src\config\config_definitions.py =====
"""
This module provides specific classes for managing configuration parameters of an experiment.
It builds upon the generic configuration management provided by the base_config module.

Classes:
    - `ConfigurationParameters`: A class representing configuration parameters for an experiment.
    - `ConfigurationPaths`: A class to manage configuration paths for an experiment.
    - `ExperimentConfiguration`: A dataclass representing the complete configuration for an experiment,
    combining configuration parameters and paths.
"""
import os

from pydantic import BaseModel, model_validator, Field

from src.config.enums import Hutch, Detector, Xray, Hertz


class ExpParams(BaseModel):
    """
    A dataclass to represent configuration parameters for an experiment.

    sdd = 1.3m
    dps = 75e-06m (75 um)
    beam_energy = 9.7 keV
    wavelength [A]
    
    Attributes:
        hutch (Hutch): The hutch setting.
        detector (Detector): The detector setting.
        xray (Xray): The x-ray setting.
        pump_setting (Hertz): The pump setting.
        x1 (int): The x1 setting.
        x2 (int): The x2 setting.
        y1 (int): The y1 setting.
        y2 (int): The y2 setting.
    """
    hutch: Hutch = Hutch.EH1
    detector: Detector = Detector.JUNGFRAU2
    xray: Xray = Xray.HARD
    pump_setting: Hertz = Hertz.FIFTEEN
    x1: int = 0
    x2: int = 1
    y1: int = 2
    y2: int = 3
    sdd: float = 1.3
    dps: float = 7.5e-5
    beam_energy: float = 10
    sigma_factor: float = 1
    wavelength: float = None

    @model_validator(mode='before')
    @classmethod
    def calculate_wavelength(cls, values):
        """Calculate Wavelength"""
        beam_energy = values.get('beam_energy')
        if beam_energy is not None:
            values['wavelength'] = 12.398419843320025 / beam_energy
        return values


class ExpPaths(BaseModel):
    """
    A class to manage configuration paths for an experiment.

    Attributes:
        load_dir (str): The load directory path.
        anaylsis_dir (str): The save directory path.
    """
    load_dir: str = ""
    anaylsis_dir: str = ""
    param_dir: str = "DataParameters"
    image_dir: str = "Images"
    mat_dir: str = "MatFiles"
    npz_dir: str = "NpzFiles"
    tif_dir: str = "TifFiles"

    @model_validator(mode='before')
    @classmethod
    def join_paths(cls, values):
        """join paths"""
        anaylsis_dir = values.get('anaylsis_dir')
        if anaylsis_dir is not None:
            values['param_dir'] = os.path.join(anaylsis_dir, values['param_dir'])
            values['image_dir'] = os.path.join(anaylsis_dir, values['image_dir'])
            values['mat_dir'] = os.path.join(anaylsis_dir, values['mat_dir'])
            values['npz_dir'] = os.path.join(anaylsis_dir, values['npz_dir'])
            values['tif_dir'] = os.path.join(anaylsis_dir, values['tif_dir'])
        return values


class ExpConfig(BaseModel):
    """
    A dataclass to represent the complete configuration for an experiment.

    Attributes:
        runs (list[int]): The run numbers.
        param (ConfigurationParameters): The configuration parameters.
        path (ConfigurationPaths): The configuration paths.
    """
    runs: list[int] = Field(default_factory=list)
    param: ExpParams = ExpParams()
    path: ExpPaths = ExpPaths()


if __name__ == "__main__":
    config_dict = {
        "runs" : ["1", "2", "3"],
        'path': {
            'load_dir': 'your\\path', 
            'anaylsis_dir': 'your\\path',
            'image_dir': 'Image', 
            'mat_dir': 'mat_files', 
            'npz_dir': 'npz_files', 
            'param_dir': 'DataParameter', 
            'tif_dir': 'tif_files'
            },
        'param': {
            'xray': 'HX',
            'detector': 'jungfrau2',
            'pump_setting': '15HZ',
            'hutch': 'eh2',
            'sdd': 1.3,
            'dps': 7.5e-05,
            'beam_energy': 9.7,
            'x1': 0, 
            'x2': 1, 
            'y1': 2, 
            'y2': 3
            }
        }

    config = ExpConfig(**config_dict)
    print(config.runs)

    print(config.path.load_dir)
    print(config.path.anaylsis_dir)
    print(config.path.param_dir)

    params = ExpParams(beam_energy=9.7)
    print(params.wavelength)


===== .\src\config\enums.py =====
"""
This module defines enumerations for various experiment configurations.
It includes enums for detectors, hutch settings, X-ray types, and Hertz settings, 
providing a structured approach to handle these configuration options.

Classes:
    - `Detector`: Enum representing different types of detectors.
    - `Hutch`: Enum representing different hutch settings.
    - `Xray`: Enum representing different X-ray types.
    - `Hertz`: Enum representing different Hertz settings.

Each enum class provides a string representation of the enum value.
"""
from enum import Enum


class Detector(Enum):
    """
    Enum representing different types of detectors.

    To add a new detector, follow these steps:

    1. Add the detector name in all uppercase:
        <DETECTOR_NAME> = '<detector_name>'
    """
    JUNGFRAU1 = 'jungfrau1'
    JUNGFRAU2 = 'jungfrau2'

class Hutch(Enum):
    """
    Enum representing different hutch settings.

    To add a new hutch, follow these steps:

    1. Add the hutch name in all uppercase:
        <HUTCH_NAME> = '<hutch_name>'
    """
    EH1 = 'eh1'
    EH2 = 'eh2'

class Xray(Enum):
    """
    Enum representing different X-ray types.
    """
    SOFT = 'SX'
    HARD = 'HX'

class Hertz(Enum):
    """
    Enum representing different Hertz settings.
    """
    ZERO = '0HZ'
    TEN = '10HZ'
    FIFTEEN = '15HZ'
    TWENTY = '20HZ'
    THIRTY = '30HZ'
    SIXTY = '60HZ'


===== .\src\gui\preprocess_gui.py =====
import os

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.widgets import Slider

from src.preprocessor.generic_preprocessors import get_linear_regression_confidence_bounds, ransac_regression
from src.processor.loader import HDF5FileLoader
from src.utils.file_util import get_run_scan_directory, get_file_list

from src.config.config import load_config
import numpy.typing as npt


def find_outliers_gui(y: npt.NDArray, x: npt.NDArray) -> float:
    fig, ax = plt.subplots(figsize=(10, 6))
    plt.subplots_adjust(left=0.1, bottom=0.25, right=0.9, top=0.9)

    # Initial plot
    sigma_init = 3.0
    lb, ub, yf = get_linear_regression_confidence_bounds(y, x, sigma_init)
    within_bounds = (y >= lb) & (y <= ub)

    normal_points, = ax.plot(
        x[within_bounds], y[within_bounds],
        'o', color='blue', markersize=3,
        label='Normal Points'
    )
    outlier_points, = ax.plot(
        x[~within_bounds], y[~within_bounds],
        'o', color='red', markersize=3,
        label='Outliers'
    )
    line_fit, = ax.plot(x, yf, color='black', label='Fitted Line')
    ax.fill_between(x, lb, ub, color='gray', alpha=0.2, label='Confidence Interval')

    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_title('Linear Regression with Confidence Interval for Outlier Detection')
    y_range = np.max(y) - np.min(y)
    ax.set_ylim([np.min(y) - y_range * 0.1, np.max(y) + y_range * 0.1])
    ax.legend(loc='lower right')

    # Add sigma value text
    sigma_text = ax.text(0.02, 0.98, f'Sigma: {sigma_init:.1f}', transform=ax.transAxes, verticalalignment='top')

    # Add outlier count text
    outlier_count = np.sum(~within_bounds)
    outlier_text = ax.text(
        0.02, 0.93,
        f'Outliers: {outlier_count} ({outlier_count/len(y):.1%})',
        transform=ax.transAxes, verticalalignment='top'
    )

    axsigma = plt.axes([0.25, 0.1, 0.65, 0.03], facecolor='lightgoldenrodyellow')
    sigma_slider = Slider(axsigma, 'Sigma', 0.1, 20.0, valinit=sigma_init, valstep=0.1)

    def update(val):
        sigma = sigma_slider.val
        lb, ub, yf = get_linear_regression_confidence_bounds(y, x, sigma)
        line_fit.set_ydata(yf)

        # Clear previous fill_between collection
        for coll in ax.collections:
            if isinstance(coll, plt.matplotlib.collections.PolyCollection):
                coll.remove()

        ax.fill_between(x, lb, ub, color='gray', alpha=0.2)

        within_bounds = (y >= lb) & (y <= ub)
        normal_points.set_data(x[within_bounds], y[within_bounds])
        outlier_points.set_data(x[~within_bounds], y[~within_bounds])

        # Update sigma text
        sigma_text.set_text(f'Sigma: {sigma:.1f}')

        # Update outlier count
        outlier_count = np.sum(~within_bounds)
        outlier_text.set_text(f'Outliers: {outlier_count} ({outlier_count/len(y):.1%})')

        fig.canvas.draw_idle()

    sigma_slider.on_changed(update)

    plt.show()

    return round(sigma_slider.val, 1)


def find_outliers_run_scan_gui(run: int, scan: int) -> float:

    config = load_config()
    scan_dir = get_run_scan_directory(config.path.anaylsis_dir, run, scan)
    files = get_file_list(scan_dir)
    file = os.path.join(scan_dir, files[len(files) // 2])

    rr = HDF5FileLoader(file)
    images = rr.images
    qbpm = rr.qbpm_sum

    return find_outliers_gui(images.sum(axis=(1, 2)), qbpm)


def RANSAC_regression_gui(run: int, scan: int) -> None:
    config = load_config()
    scan_dir = get_run_scan_directory(config.path.anaylsis_dir, run, scan)
    files = get_file_list(scan_dir)
    file = os.path.join(scan_dir, files[len(files) // 2])

    rr = HDF5FileLoader(file)
    images = rr.images
    qbpm = rr.qbpm_sum

    intensities = images.sum(axis=(1, 2))
    mask, coef, intercept = ransac_regression(intensities, qbpm)
    plt.scatter(qbpm[mask], intensities[mask], color="blue", label="Inliers")
    plt.scatter(qbpm[~mask], intensities[~mask], color="red", label="Outliers")
    plt.title("RANSAC - outliers vs inliers")

    plt.show()


if __name__ == "__main__":
    # Example data
    np.random.seed(0)  # For reproducibility
    x = np.random.normal(10, 5, 100)
    x.sort()
    y = 2.5 * x + np.random.normal(0, 2, 100)
    y[0] += 10  # Add an outlier
    y[-1] -= 10  # Add another outlier

    sigma = find_outliers_gui(x, y)
    print(sigma)


===== .\src\gui\roi.py =====
from typing import Optional

import matplotlib.pyplot as plt
from matplotlib import patches
import numpy as np
import numpy.typing as npt
from roi_rectangle import RoiRectangle

from src.processor.loader import get_hdf5_images
from src.utils.file_util import get_run_scan_directory, get_file_list
from src.config.config import load_config, ExpConfig


class RoiSelector:
    def __init__(self):
        self.drawing = False
        self.ix, self.iy = -1, -1
        self.fx, self.fy = -1, -1
        self.rect = None
        self.ax = None

    def on_mouse_press(self, event):

        if event.inaxes is not None:
            if event.button == 1:
                self.drawing = True
                self.ix, self.iy = int(event.xdata), int(event.ydata)
                self.fx, self.fy = self.ix, self.iy
                if self.rect is not None:
                    self.rect.remove()
                self.rect = patches.Rectangle((self.ix, self.iy), 1, 1, linewidth=1, edgecolor='r', facecolor='none')
                self.ax.add_patch(self.rect)
                plt.draw()

    def on_mouse_release(self, event):

        if event.inaxes is not None and self.drawing:
            self.drawing = False
            self.fx, self.fy = int(event.xdata), int(event.ydata)
            if self.rect is not None:
                self.rect.set_width(self.fx - self.ix)
                self.rect.set_height(self.fy - self.iy)
                plt.draw()

    def on_mouse_move(self, event):

        if event.inaxes is not None and self.drawing:
            self.fx, self.fy = int(event.xdata), int(event.ydata)
            if self.rect is not None:
                self.rect.set_width(self.fx - self.ix)
                self.rect.set_height(self.fy - self.iy)
                plt.draw()

    def select_roi(self, image: npt.NDArray) -> Optional[tuple[int, int, int, int]]:
        if image.ndim != 2:
            raise TypeError(f"Invalid shape {image.shape} for image data")
        fig, self.ax = plt.subplots()
        self.ax.imshow(image)

        fig.canvas.mpl_connect('button_press_event', self.on_mouse_press)
        fig.canvas.mpl_connect('button_release_event', self.on_mouse_release)
        fig.canvas.mpl_connect('motion_notify_event', self.on_mouse_move)

        plt.show()

        if self.ix == -1 or self.iy == -1 or self.fx == -1 or self.fy == -1:
            return None
        x1, y1 = min(self.ix, self.fx), min(self.iy, self.fy)
        x2, y2 = max(self.ix, self.fx), max(self.iy, self.fy)
        return (x1, y1, x2, y2)


def select_roi_by_run_scan(run: int, scan: int, index_mode: Optional[int] = None) -> Optional[RoiRectangle]:
    config = load_config()
    load_dir = config.path.load_dir
    scan_dir = get_run_scan_directory(load_dir, run, scan)
    files = get_file_list(scan_dir)

    if index_mode is None:
        index = len(files) // 2
    elif isinstance(index_mode, int):
        index = index_mode

    images = get_hdf5_images(files[index], config)
    image = np.log1p(images.sum(axis=0))
    roi = RoiSelector().select_roi(image)
    if roi is None:
        return None
    return RoiRectangle(*roi)


def get_roi_auto(
    image,
    width: int = 5,
) -> RoiRectangle:
    """get roi_rect by max pixel"""
    center = np.unravel_index(np.argmax(image), image.shape)[::-1]
    return RoiRectangle(center[0] - width, center[1] - width, center[0] + width, center[1] + width)


if __name__ == "__main__":

    images = get_single_images_from_hdf5(201, 1, 1)

    image = images.sum(axis=0)
    roi_rect = get_roi_auto(image)
    print(roi_rect)
    roi = RoiSelector().select_roi(np.log1p(image))

    print(roi)


===== .\src\inspection\file_status_inspector.py =====
import os
from typing import Any

import pandas as pd
import h5py

from src.utils.file_util import get_file_list, get_folder_list


def get_file_status(root: str) -> dict:

    status: dict = {}

    runs: list[str] = get_folder_list(root)
    for run in runs:
        path = os.path.join(root, run)
        scans = get_folder_list(path)
        for scan in scans:
            path = os.path.join(path, scan)
            files = get_file_list(path)

            name = "_".join(path[-2:])[:-2]

            nums = {int(file[1:-3]) for file in files}
            max_num = max(nums)
            missing_nums = set(range(1, max_num + 1)) - nums
            status[name] = [max_num, missing_nums]

    return status


def h5_tree(val: Any, pre: None = '') -> None:
    """
    with h5py.File(file) as hf:
        print(hf)
        h5_tree(hf)
    """
    items_cnt = len(val)
    for key, val in val.items():
        items_cnt -= 1
        if items_cnt == 0:
            # the last item
            if isinstance(val, h5py._hl.group.Group):
                print(pre + '└── ' + key)
                h5_tree(val, pre + '    ')
            else:
                try:
                    print(pre + '└── ' + key + ' (%d)' % len(val))
                except TypeError:
                    print(pre + '└── ' + key + ' (scalar)')
        else:
            if isinstance(val, h5py._hl.group.Group):
                print(pre + '├── ' + key)
                h5_tree(val, pre + '│   ')
            else:
                try:
                    print(pre + '├── ' + key + ' (%d)' % len(val))
                except TypeError:
                    print(pre + '├── ' + key + ' (scalar)')


def load_matdata(h5file: str) -> pd.DataFrame:
    return pd.read_hdf(h5file, 'metadata')


if __name__ == "__main__":

    from utils.file_util import get_run_scan_directory
    from src.config.config import load_config

    config = load_config()
    load_dir = config.path.load_dir

    file = get_run_scan_directory(load_dir, 122, 1, 30)

    # metadata = load_matdata(file)
    # metadata.to_csv("metadata122.csv")

    with h5py.File(file) as hf:
        print(hf)
        h5_tree(hf)


===== .\src\inspection\list_files.py =====
import os


def gather_python_files(directory: str, output_file: str):
    """Gather every texts in .py files and save to txt file."""
    total_length = 0
    with open(output_file, 'w', encoding='utf-8') as outfile:
        for root, _, files in os.walk(directory):
            for file in files:
                if file.endswith('.py'):
                    file_path = os.path.join(root, file)
                    outfile.write(f"===== {file_path} =====\n")
                    with open(file_path, 'r', encoding='utf-8') as infile:
                        texts = infile.read()
                        total_length += len(texts)
                        outfile.write(texts)
                    outfile.write("\n\n")
    print(total_length)


if __name__ == "__main__":
    project_directory: str = '.\\'
    output_file: str = 'project_code.txt'
    gather_python_files(project_directory, output_file)


===== .\src\inspection\profiler.py =====
import cProfile
import pstats
import io
import logging

from src.processor.loader import HDF5FileLoader
from src.config.config import load_config, ExpConfig
from src.utils.file_util import get_run_scan_directory


def main() -> None:
    """Profile program with cProfile module and visualize with tuna."""
    config: ExpConfig = load_config()
    load_dir: str = config.path.load_dir
    file: str = get_run_scan_directory(load_dir, 150, 1, 31)

    logging_file: str = 'logs\\profiling\\profiling.log'
    # logging Setting
    logging.basicConfig(filename=logging_file, level=logging.INFO, format='%(message)s')

    # Create a profiler object
    profiler = cProfile.Profile()

    # Enable the profiler
    profiler.enable()

    # Run the main function from the other file
    HDF5FileLoader(file)
    # processing_main.main()

    # Disable the profiler
    profiler.disable()

    # Create a Stats object and sort the results by cumulative time
    stats = pstats.Stats(profiler)
    stats.strip_dirs()
    stats.sort_stats('cumulative')

    # Redirect the stats output to a StringIO object
    output_stream = io.StringIO()
    stats.stream = output_stream

    # Print the stats to the StringIO object
    stats.print_stats()

    # Get the captured output
    profiling_results = output_stream.getvalue()

    # Log the captured output
    logging.info(profiling_results)

    # Save the profiling results to a file
    stats_file = 'logs\\profiling\\profiling.prof'
    stats.dump_stats(stats_file)

    print(f"Profiling results logged to '{logging_file}'")


if __name__ == "__main__":
    main()


===== .\src\inspection\project_overview.py =====
import os
from collections import defaultdict

import ast
import radon.complexity as rc
import radon.raw as rr
from radon.complexity import cc_rank


def analyze_project(root_dir):
    project_structure = defaultdict(lambda: {"files": [], "modules": {}})

    for dirpath, _, filenames in os.walk(root_dir):
        rel_path = os.path.relpath(dirpath, root_dir)
        if rel_path == ".":
            rel_path = ""

        for filename in filenames:
            if filename.endswith(".py"):
                file_path = os.path.join(dirpath, filename)
                module_name = os.path.splitext(filename)[0]

                with open(file_path, "r", encoding="utf-8") as file:
                    content = file.read()

                tree = ast.parse(content)
                classes, functions = [], []

                for node in ast.walk(tree):
                    if isinstance(node, ast.ClassDef):
                        classes.append(node.name)
                    elif isinstance(node, ast.FunctionDef):
                        functions.append(node.name)

                loc = len(content.splitlines())

                project_structure[rel_path]["files"].append(filename)
                project_structure[rel_path]["modules"][module_name] = {
                    "classes": classes,
                    "functions": functions,
                    "loc": loc
                }

    return project_structure


def print_project_structure(structure, indent=""):
    for dir_name, dir_content in structure.items():
        print(f"{indent}└── {dir_name}/")

        for module_name, module_info in dir_content["modules"].items():
            print(f"{indent}    ├── {module_name}.py:")
            print(f"{indent}    │   ├── Classes: {', '.join(module_info['classes'])}")
            print(f"{indent}    │   ├── Functions: {', '.join(module_info['functions'])}")
            # print(f"{indent}    │   └── Lines of Code: {module_info['loc']}")

        # if dir_content["files"]:
        #     print(f"{indent}    └── Files: {', '.join(dir_content['files'])}")


def analyze_code_complexity(root_dir):
    complexity_data = {}

    for dirpath, _, filenames in os.walk(root_dir):
        rel_path = os.path.relpath(dirpath, root_dir)
        if rel_path == ".":
            rel_path = ""

        for filename in filenames:
            if filename.endswith(".py"):
                file_path = os.path.join(dirpath, filename)


                with open(file_path, "r", encoding="utf-8") as file:
                    content = file.read()

                raw_metrics = rr.analyze(content)
                cc_metrics = rc.cc_visit(content)

                complexity_data[file_path] = {
                    "loc": raw_metrics.loc,
                    "lloc": raw_metrics.lloc,
                    "sloc": raw_metrics.sloc,
                    "comments": raw_metrics.comments,
                    "multi": raw_metrics.multi,
                    "single_comments": raw_metrics.single_comments,
                    "cc_complexity": {
                        "average": calculate_average_complexity(cc_metrics),
                        "details": [{"name": node.name, "complexity": node.complexity} for node in cc_metrics]
                    }
                }

    return complexity_data


def calculate_average_complexity(cc_metrics):
    total_complexity = sum(node.complexity for node in cc_metrics)
    return total_complexity / len(cc_metrics) if cc_metrics else 0


def print_code_complexity(complexity_data):
    for file_path, data in complexity_data.items():
        print(f"{file_path}:")
        print(f"  Lines of Code (LOC): {data['loc']}")
        print(f"  Logical Lines of Code (LLOC): {data['lloc']}")
        print(f"  Source Lines of Code (SLOC): {data['sloc']}")
        print(f"  Comments: {data['comments']}")
        print(f"  Multi-line Comments: {data['multi']}")
        print(f"  Single-line Comments: {data['single_comments']}")
        print("  Cyclomatic Complexity (CC):")
        print(f"    Average: {data['cc_complexity']['average']}")
        print("    Details:")
        for detail in data['cc_complexity']['details']:
            print(f"      {detail['name']}: {detail['complexity']}")
        print()


def print_complexity_grades(root_dir):

    for dirpath, _, filenames in os.walk(root_dir):
        rel_path = os.path.relpath(dirpath, root_dir)
        if rel_path == ".":
            rel_path = ""

        for filename in filenames:
            if filename.endswith(".py"):
                file_path = os.path.join(dirpath, filename)

                with open(file_path, "r", encoding="utf-8") as file:
                    content = file.read()

                cc_metrics = rc.cc_visit(content)

                print(f"{file_path}:")
                for node in cc_metrics:
                    grade = cc_rank(node.complexity)
                    print(f"  {node.name}: {node.complexity} ({grade})")
                print()


if __name__ == "__main__":
    root_dir: str = ".\\"

    project_structure = analyze_project(root_dir)
    print("Project Structure:")
    print_project_structure(project_structure)

    # print("Project Complexity Grades:")
    # print_complexity_grades(root_dir)

    # complexity_data = analyze_code_complexity(root_dir)
    # print("Code Complexity Analysis:")
    # print_code_complexity(complexity_data)


===== .\src\preprocessor\generic_preprocessors.py =====
import os
from typing import Optional

import numpy as np
import numpy.typing as npt
from scipy.optimize import curve_fit
from sklearn.linear_model import RANSACRegressor

from src.config.config import load_config


def ransac_regression(y: np.ndarray, x: np.ndarray, min_samples: Optional[int] = None) -> tuple[npt.NDArray[np.bool_], npt.NDArray, npt.NDArray]:
    """
    Perform RANSAC (Random Sample Consensus) regression to identify inliers and estimate the regression model.

    Parameters:
    - y (np.ndarray): The target variable array.
    - x (np.ndarray): The feature variable array.
    - min_samples (int, optional): The minimum number of samples to fit the model. Default is 3.

    Returns:
    - tuple[npt.NDArray[np.bool_], npt.NDArray, npt.NDArray]: A tuple containing the inlier mask, coefficient, and intercept of the linear model.
    """
    X = x[:, np.newaxis]
    ransac = RANSACRegressor(min_samples=min_samples).fit(X, y)
    inlier_mask = ransac.inlier_mask_
    return inlier_mask, ransac.estimator_.coef_, ransac.estimator_.intercept_


def get_linear_regression_confidence_bounds(
    y: npt.NDArray,
    x: npt.NDArray,
    sigma: float
) -> npt.NDArray:
    """
    Get lower and upper bounds for data points based on their confidence interval in a linear regression model.

    Statistical Explanation:
    This function applies the principle of propagation of uncertainty. It fits a linear
    regression model y = mx + b, calculates the standard errors of the model parameters
    m and b, and uses these to generate prediction intervals for identifying outliers.

    The prediction interval is calculated using the formula:
    y ± sqrt((m_err * x)^2 + b_err^2) * sigma
    where m_err and b_err are the standard errors of m and b respectively.

    Parameters:
    y (NDArray): Dependent variable data. Shape: (N,)
    x (NDArray): Independent variable data. Shape: (N,)
    sigma (float): Number of standard deviations for the confidence interval. Default is 3.0.

    Returns:
    lowerbound (NDArray): Lower bounds of y
    upperbound (NDArray): Upper bounds of y
    y_fit (NDArray): Fitted y

    Note:
    This method assumes a linear relationship in the data. For strong non-linearities,
    a different approach may be necessary.
    """
    def linear_model(x, m, b):
        return m * x + b

    params, covars = curve_fit(linear_model, x, y)

    m, b = params
    m_err, b_err = np.sqrt(np.diag(covars))
    y_fit = linear_model(x, m, b)

    # Calculate upper and lower bounds considering both slope and intercept errors
    error = np.sqrt((m_err * x)**2 + b_err**2)
    upper_bound = y_fit + error * sigma
    lower_bound = y_fit - error * sigma

    return lower_bound, upper_bound, y_fit


def filter_images_qbpm_by_linear_model(images: npt.NDArray, qbpm: npt.NDArray, sigma: float) -> tuple[npt.NDArray, npt.NDArray]:
    """
    Filter images based on the confidence interval of their intensities using a linear regression model with QBPM values.

    This function computes the total intensity of each image, applies a linear regression model
    to the intensity and QBPM values, and generates a mask to filter out images whose intensities
    fall outside the specified confidence interval.

    Parameters:
    images (NDArray): Array of images. Shape: (N, H, W), where N is the number of images, and H and W are the height and width of each image.
    qbpm (NDArray): Array of QBPM (Quadrature Balanced Photodetector Measurements) values. Shape: (N,)
    sigma (float): Number of standard deviations for the confidence interval.

    Returns:
    tuple[NDArray, NDArray]: Tuple of filtered intensities and QBPM values.
        - Filtered intensities (NDArray): Array of intensities within the confidence interval. Shape: (M,), where M <= N.
        - Filtered qbpm (NDArray): Array of QBPM values corresponding to the filtered intensities. Shape: (M,), where M <= N.

    Note:
    This method uses the `get_linear_regression_confidence_lower_upper_bound` function to generate the mask based
    on the linear regression model and confidence interval.
    """
    intensites = images.sum(axis=(1, 2))
    lower_bound, upper_bound, _ = get_linear_regression_confidence_bounds(intensites, qbpm, sigma)
    mask = np.logical_and(intensites >= lower_bound, intensites <= upper_bound)

    return images[mask], qbpm[mask]


def div_images_by_qbpm(images: npt.NDArray, qbpm: npt.NDArray) -> npt.NDArray:
    """
    Divide images by qbpm.

    Parameters:
    images (NDArray): Array of images. Shape: (N, H, W), where N is the number of images, and H and W are the height and width of each image.
    qbpm (NDArray): Array of QBPM (Quadrature Balanced Photodetector Measurements) values. Shape: (N,)

    Returns:
    NDArray: Images that divided by qbpm
    """
    return images * qbpm.mean() / qbpm[:, np.newaxis, np.newaxis]


def subtract_dark(images: npt.NDArray) -> npt.NDArray:
    config = load_config()
    dark_file = os.path.join(config.path.anaylsis_dir, "DARK\\dark.npy")

    if not os.path.exists(dark_file):
        raise FileNotFoundError(f"No such file or directory: {dark_file}")

    dark_images = np.load(dark_file)
    dark = np.mean(dark_images, axis=0)
    return np.maximum(images - dark[np.newaxis, :, :], 0)
    # return images - dark[np.newaxis, :, :]


def add_bias(images: npt.NDArray):
    bias = np.min(images)
    return images - bias


def equalize_brightness(images: np.ndarray) -> np.ndarray:
    """
    Equalize the brightness of each image in the 3D array while maintaining the overall average brightness.

    Parameters:
    - images: np.ndarray, 3D array of images (num_images, height, width).

    Returns:
    - np.ndarray: The brightness-equalized images.
    """
    intensites = images.sum(axis=(1, 2))
    overal_normed_intensites = intensites / intensites.mean()
    equalized_images = images / overal_normed_intensites[:, np.newaxis, np.newaxis]

    return equalized_images


===== .\src\preprocessor\image_qbpm_preprocessor.py =====
from functools import partial, reduce
from typing import Callable

import numpy.typing as npt
from roi_rectangle import RoiRectangle

from src.preprocessor.generic_preprocessors import (
    div_images_by_qbpm,
    filter_images_qbpm_by_linear_model,
    subtract_dark,
    ransac_regression,
    equalize_brightness,
    add_bias
)


ImagesQbpm = tuple[npt.NDArray, npt.NDArray]
ImagesQbpmProcessor = Callable[[ImagesQbpm], ImagesQbpm]


def shift_to_positive(images_qbpm: ImagesQbpm) -> ImagesQbpm:
    """
    Shift the images to ensure all values are non-negative by adding a bias.

    This function adds a bias to the images to ensure that all pixel values are non-negative.
    The bias is calculated as the absolute value of the minimum pixel value in the images.

    Parameters:
    - images_qbpm (tuple[Images, Qbpm]): tuple of Images and Qbpm

    Returns:
    - tuple[Images, Qbpm]: A tuple containing the shifted images and the original QBPM values.
    """
    return add_bias(images_qbpm[0]), images_qbpm[1]


def subtract_dark_background(images_qbpm: ImagesQbpm) -> ImagesQbpm:
    """
    Remove the dark background from the images.

    Parameters:
    - images_qbpm (tuple[Images, Qbpm]): tuple of Images and Qbpm

    Returns:
    - tuple[Images, Qbpm]: The images with dark background removed and the original Qbpm values.
    """

    return subtract_dark(images_qbpm[0]), images_qbpm[1]


def normalize_images_by_qbpm(images_qbpm: ImagesQbpm) -> ImagesQbpm:
    """
    Normalize the images by the Qbpm values.

    Parameters:
    - images_qbpm (tuple[Images, Qbpm]): tuple of Images and Qbpm

    Returns:
    - tuple[Images, Qbpm]: The normalized images and the original Qbpm values.
    """
    return div_images_by_qbpm(images_qbpm[0], images_qbpm[1]), images_qbpm[1]


def remove_outliers_using_ransac(images_qbpm: ImagesQbpm) -> ImagesQbpm:
    """
    Remove outliers from the images and Qbpm values using RANSAC regression.

    Parameters:
    - images_qbpm (tuple[Images, Qbpm]): tuple of Images and Qbpm

    Returns:
    - tuple[Images, Qbpm]: The images and Qbpm values with outliers removed.
    """
    mask = ransac_regression(images_qbpm[0].sum(axis=(1, 2)), images_qbpm[1], min_samples=2)[0]
    return images_qbpm[0][mask], images_qbpm[1][mask]


def create_ransac_roi_outlier_remover(roi_rect: RoiRectangle) -> ImagesQbpmProcessor:
    """
    Create a function to remove outliers using a linear model with a given sigma.

    Parameters:
    - sigma: float, the sigma value for the outlier removal.

    Returns:
    - ImageQbpmProcessor: A function that takes ImagesQbpm and returns the filtered ImagesQbpm.
    """
    def remove_ransac_roi_outliers(images_qbpm: ImagesQbpm) -> ImagesQbpm:
        roi_image = roi_rect.slice(images_qbpm[0])
        mask = ransac_regression(roi_image.sum(axis=(1, 2)), images_qbpm[1], min_samples=2)[0]
        return images_qbpm[0][mask], images_qbpm[1][mask]
    return remove_ransac_roi_outliers


def equalize_intensities(images_qbpm: ImagesQbpm) -> ImagesQbpm:
    """
    Equalize the intensities of the images while keeping the Qbpm values unchanged.

    Parameters:
    - images_qbpm (tuple[Images, Qbpm]): tuple of Images and Qbpm

    Returns:
    - tuple[np.ndarray, np.ndarray]: A tuple containing the brightness-equalized images and the original Qbpm values.
    """
    return equalize_brightness(images_qbpm[0]), images_qbpm[1]


def create_linear_model_outlier_remover(sigma) -> ImagesQbpmProcessor:
    """
    Create a function to remove outliers using a linear model with a given sigma.

    Parameters:
    - sigma: float, the sigma value for the outlier removal.

    Returns:
    - ImageQbpmProcessor: A function that takes ImagesQbpm and returns the filtered ImagesQbpm.
    """
    remove_outlier: ImagesQbpmProcessor = partial(filter_images_qbpm_by_linear_model, sigma=sigma)
    return remove_outlier


def compose(*funcs: Callable):
    """combine multiple functions"""
    return reduce(lambda f, g: lambda x: f(g(x)), funcs)


===== .\src\preprocessor\remove_continuous_noise.py =====
import numpy as np
import numpy.typing as npt


def remove_noise(images: npt.NDArray, threshold: float, front: int = 5, back: int = 5):
    N = 15
    n_th = N * 0.6

    images_front = images[:front]
    array_sel_th = images_front >= threshold
    noise_addr = np.prod(array_sel_th, axis=0)
    noise_avg = np.mean(images_front * noise_addr, axis=0)
    array_sel_noise_removal = images - noise_avg

    array_sel_2 = array_sel_noise_removal[-back:]
    array_sel_th_2 = array_sel_2 >= threshold
    noise_addr_2 = np.sum(array_sel_th_2, axis=0) >= n_th

    noise_avg_2 = np.mean(array_sel_2 * noise_addr_2, axis=0)
    array_sel_noise_removal_2 = array_sel_noise_removal - noise_avg_2
    array_sel_noise_removal = np.abs(array_sel_noise_removal_2)

    return array_sel_noise_removal


if __name__ == "__main__":
    from scipy.io import loadmat, savemat
    from roi_rectangle import RoiRectangle

    file = "Y:\\240608_FXS\\raw_data\\h5\\type=raw\\Mat_files2\\run=0176_scan=0001_no_normalize_poff.mat"
    images = loadmat(file)["data"]
    images = images.swapaxes(0, 2)
    images = images.swapaxes(1, 2)

    denoised_images = remove_noise(images, 5)

    savemat("denoised_images.mat")

    roi_rect = RoiRectangle(0, 0, 500, 500)
    roi_rect.slice(images)


===== .\src\processor\core.py =====
import os
from collections import defaultdict
from typing import Optional, DefaultDict, Type, Any

import numpy as np
import numpy.typing as npt
from tqdm import tqdm

from src.utils.file_util import get_file_list
from src.processor.saver import SaverStrategy
from src.processor.loader import RawDataLoader
from src.preprocessor.image_qbpm_preprocessor import ImagesQbpmProcessor
from src.logger import setup_logger, Logger
from src.config.config import load_config, ExpConfig


class CoreProcessor:
    """
    Use ETL Pattern
    """
    def __init__(
        self,
        LoaderStrategy: Type[RawDataLoader],
        preprocessor: Optional[dict[str, ImagesQbpmProcessor]] = None,
        logger: Optional[Logger] = None
    ) -> None:

        self.LoaderStrategy: Type[RawDataLoader] = LoaderStrategy
        self.preprocessor: dict[str, ImagesQbpmProcessor] = preprocessor if preprocessor is not None else {"no_processing": lambda x: x}
        self.preprocessor_data_dict: dict[str, DefaultDict[str, list]] = {
            pipline_name: defaultdict(list)
            for pipline_name in self.preprocessor
        }

        self.logger: Logger = logger if logger is not None else setup_logger()
        self.config: ExpConfig = load_config()
        self.result: dict[str, DefaultDict[str, npt.NDArray]] = {}
        self.logger.info(f"Meta Data:\n{self.config}")

    def scan(self, scan_dir: str):
        """
        Scans directories for rocking scan data and processes them.

        Parameters:
        - run_num (int): Run number to scan.
        """

        self.logger.info(f"Starting scan: {scan_dir}")

        self.result: dict[str, DefaultDict[str, npt.NDArray]] = self.process_scan_directory(scan_dir)
        self.logger.info(f"Completed processing: {scan_dir}")

    def process_scan_directory(self, scan_dir: str) -> dict[str, DefaultDict[str, npt.NDArray]]:
        """
        Processes a single scan directory.

        Parameters:
        - scan_dir (str): Directory path of the scan to process.

        Returns:
        - dict[str, npt.NDArray]: Dictionary containing stacked images from the scan.
        """
        self.logger.info(f"Starting single scan for directory: {scan_dir}")

        hdf5_files = get_file_list(scan_dir)
        pbar = tqdm(hdf5_files, total=len(hdf5_files))

        for hdf5_file in pbar:

            loader_strategy = self.get_loader(scan_dir, hdf5_file)
            if loader_strategy is not None:
                self.add_processed_data_to_dict(loader_strategy)

        return self.stack_processed_data(self.preprocessor_data_dict)

    def get_loader(self, scan_dir: str, hdf5_file: str) -> Optional[RawDataLoader]:
        """Get Loader"""
        hdf5_dir = os.path.join(scan_dir, hdf5_file)
        try:
            return self.LoaderStrategy(hdf5_dir)
        except KeyError as e:
            self.logger.warning(f"{e}")
            self.logger.warning(f"KeyError happened in {hdf5_dir}")
            return None
        except FileNotFoundError as e:
            self.logger.warning(f"{e}")
            self.logger.warning(f"FileNotFoundError happened in {hdf5_dir}")
            return None
        # except Exception as e:
        #     self.logger.exception(f"Failed to load: {type(e)}: {str(e)}")
        #     import traceback
        #     error_message = traceback.format_exc()
        #     self.logger.exception(error_message)
        #     return None

    def add_processed_data_to_dict(self, loader_strategy: RawDataLoader) -> dict[str, DefaultDict[str, list]]:

        preprocessor_data: dict[str, dict[str, Any]] = {}
        for preprocessor_name, preprocessor in self.preprocessor.items():

            data: dict[str, Any] = {}

            images_dict = loader_strategy.get_data()
            if "pon" in images_dict:
                applied_images: npt.NDArray = preprocessor((images_dict['pon'], images_dict['pon_qbpm']))[0]
                data['pon'] = applied_images.mean(axis=0)
            if 'poff' in images_dict:
                applied_images: npt.NDArray = preprocessor((images_dict['poff'], images_dict['poff_qbpm']))[0]
                data['poff'] = applied_images.mean(axis=0)

            data["delay"] = loader_strategy.delay
            preprocessor_data[preprocessor_name] = data

        for preprocessor_name, data in preprocessor_data.items():
            for data_name, data_value in data.items():
                self.preprocessor_data_dict[preprocessor_name][data_name].append(data_value)

    def stack_processed_data(self, preprocessor_data_dict: dict[str, DefaultDict[str, list]]) -> dict[str, DefaultDict[str, npt.NDArray]]:
        preprocessor_data_dict_result: dict[str, DefaultDict[str, npt.NDArray]] = {}
        for preprocessor_name, data in preprocessor_data_dict.items():
            preprocessor_data_dict_result[preprocessor_name] = {data_name: np.stack(data_list) for data_name, data_list in data.items()}

        return preprocessor_data_dict_result

    def save(self, saver: SaverStrategy, file_name: str):
        """
        Saves processed images using a specified saving strategy.

        Parameters:
        - saver (SaverStrategy): Saving strategy to use.
        - comment (str, optional): Comment to append to the file name.
        """
        self.logger.info(f"Start to save as {saver.file_type.capitalize()}")

        if not self.result:
            self.logger.error("Nothing to save")
            raise ValueError("Nothing to save")

        for pipline_name, data_dict in self.result.items():
            file_base_name = f"{file_name}"

            saver.save(file_base_name, data_dict)
            self.logger.info(f"Finished preprocessor: {pipline_name}")
            self.logger.info(f"Data Dict Keys: {data_dict.keys()}")
            self.logger.info(f"Saved file '{saver.file}'")


if __name__ == "__main__":

    from src.processor.loader import HDF5FileLoader
    from src.processor.saver import SaverFactory
    from src.preprocessor.image_qbpm_preprocessor import (
        compose,
        subtract_dark_background,
        normalize_images_by_qbpm,
        remove_outliers_using_ransac,
        equalize_intensities
    )

    run_num: int = 1
    scan_num: int = 1
    logger: Logger = setup_logger()

    # preprocessor 1
    preprocessor_normalize_images_by_qbpm: ImagesQbpmProcessor = compose(
        subtract_dark_background,
        remove_outliers_using_ransac,
        normalize_images_by_qbpm,
    )
    logger.info("preprocessor: normalize_images_by_qbpm")
    logger.info("preprocessing: subtract_dark_background")
    logger.info("preprocessing: remove_by_ransac")
    logger.info("preprocessing: normalize_images_by_qbpm")

    # preprocessor 2
    preprocessor_equalize_intensities: ImagesQbpmProcessor = compose(
        subtract_dark_background,
        remove_outliers_using_ransac,
        equalize_intensities,
    )
    logger.info("preprocessor: normalize_images_by_qbpm")
    logger.info("preprocessing: subtract_dark_background")
    logger.info("preprocessing: remove_by_ransac")
    logger.info("preprocessing: equalize_intensities")

    # preprocessor 3
    preprocessor_no_normalize: ImagesQbpmProcessor = compose(
        subtract_dark_background,
        remove_outliers_using_ransac,
    )
    logger.info("preprocessor: no_normalize")
    logger.info("preprocessing: subtract_dark_background")
    logger.info("preprocessing: remove_by_ransac")

    preprocessors: dict[str, ImagesQbpmProcessor] = {
        "normalize_images_by_qbpm": preprocessor_normalize_images_by_qbpm,
        "equalize_intensities": preprocessor_equalize_intensities,
        "no_normalize": preprocessor_no_normalize
    }

    cp = CoreProcessor(HDF5FileLoader, preprocessors, logger)
    cp.scan(run_num)

    file_name: str = f"run={run_num:0>4}_scan={scan_num:0>4}"

    mat_saver: SaverStrategy = SaverFactory.get_saver("mat")
    # tif_saver: SaverStrategy = SaverFactory.get_saver("tif")
    # npz_saver: SaverStrategy = SaverFactory.get_saver("npz")
    cp.save(mat_saver, file_name)
    # cp.save(tif_saver)
    # cp.save(npz_saver)

    logger.info("Processing is over")


===== .\src\processor\loader.py =====
import os
from abc import ABC, abstractmethod
from typing import Union

import numpy as np
import numpy.typing as npt
import pandas as pd
import h5py
import hdf5plugin  # pylint: disable=unused-import
from src.config.config import load_config, ExpConfig
from src.config.enums import Hertz


class RawDataLoader(ABC):
    @abstractmethod
    def __init__(self, file: str) -> None:
        pass

    @abstractmethod
    def get_data(self) -> dict[str, npt.NDArray]:
        pass


class HDF5FileLoader(RawDataLoader):

    def __init__(self, file: str):
        """
        Initializes the HDF5FileLoader by loading
        metadata, images, and qbpm data from the given file.

        Parameters:
        - file (str): Path to the HDF5 file.
        """
        if not os.path.isfile(file):
            raise FileNotFoundError(f"No such file: {file}")

        self.file: str = file
        self.config: ExpConfig = load_config()

        metadata: pd.DataFrame = pd.read_hdf(self.file, key='metadata')
        merged_df: pd.DataFrame = self.get_merged_df(metadata)

        # Fill Negative Values to Zero
        self.images: npt.NDArray[np.float32] = np.maximum(0, np.stack(merged_df['image'].values))
        self.qbpm: npt.NDArray[np.float32] = np.stack(merged_df['qbpm'].values)
        self.pump_status: npt.NDArray[np.bool_] = self.get_pump_mask(merged_df)
        self.delay: Union[np.float32, float] = self.get_delay(metadata)

        # roi_coord = np.array(self.metadata[f'detector_{self.config.param.hutch}_{self.config.param.detector}_parameters.ROI'].iloc[0][0])
        # roi = np.array([roi_coord[self.config.param.x1], roi_coord[self.config.param.x2], roi_coord[self.config.param.y1], roi_coord[self.config.param.y2]], dtype=np.int_)
        # self.roi_rect = RoiRectangle().from_tuple(roi)

    def get_merged_df(self, metadata: pd.DataFrame) -> pd.DataFrame:
        """
        Merges image and qbpm data with metadata based on timestamps.

        Parameters:
        - metadata (pd.DataFrame): Metadata DataFrame.

        Returns:
        - pd.DataFrame: Merged DataFrame containing metadata, images, and qbpm data.
        """
        with h5py.File(self.file, "r") as hf:
            if "detector" not in hf:
                raise KeyError("Key 'detector' not found in the HDF5 file")

            image_group = hf[f'detector/{self.config.param.hutch.value}/{self.config.param.detector.value}/image']
            images_ts = np.asarray(image_group["block0_items"], dtype=np.int64)
            images = np.asarray(image_group["block0_values"], dtype=np.float32)

            qbpm_group = hf[f'qbpm/{self.config.param.hutch.value}/qbpm1']
            qbpm_ts = np.asarray(qbpm_group['waveforms.ch1/axis1'], dtype=np.int64)
            qbpm_sum = np.stack(
                [qbpm_group[f'waveforms.ch{i + 1}/block0_values'] for i in range(4)],
                axis=0
            ).sum(axis=(0, 2))

        image_df = pd.DataFrame(
            {
                "timestamp": images_ts,
                "image": list(images)
            }
        ).set_index('timestamp')

        qbpm_df = pd.DataFrame(
            {
                "timestamp": qbpm_ts,
                "qbpm": list(qbpm_sum)
            }
        ).set_index('timestamp')

        merged_df = pd.merge(image_df, qbpm_df, left_index=True, right_index=True, how='inner')
        return pd.merge(metadata, merged_df, left_index=True, right_index=True, how='inner')

    def get_delay(self, metadata: pd.DataFrame) -> Union[np.float32, float]:
        """
        Retrieves the delay value from the metadata.

        Parameters:
        - metadata (pd.DataFrame): Metadata DataFrame.

        Returns:
        - Union[np.float32, float]: Delay value or NaN if not found.
        """
        if "th_value" in metadata:
            return np.asarray(metadata['th_value'], dtype=np.float32)[0]
        if "delay_value" in metadata:
            return np.asarray(metadata['delay_value'], dtype=np.float32)[0]
        return np.nan

    def get_pump_mask(self, merged_df: pd.DataFrame) -> npt.NDArray[np.bool_]:
        """
        Generates a pump status mask based on the configuration settings.

        Parameters:
        - merged_df (pd.DataFrame): Merged DataFrame.

        Returns:
        - npt.NDArray[np.bool_]: Pump status mask.
        """
        if self.config.param.pump_setting is Hertz.ZERO:
            return np.zeros(merged_df.shape[0], dtype=np.bool_)
        return merged_df[f'timestamp_info.RATE_{self.config.param.xray.value}_{self.config.param.pump_setting.value}'].astype(np.bool_)

    def get_data(self) -> dict[str, npt.NDArray]:
        """
        Retrieves data based on pump status.

        Returns:
        - dict[str, npt.NDArray]: Dictionary containing images and qbpm data for both pump-on and pump-off states.
        """
        data = {}

        poff_images = self.images[~self.pump_status]
        poff_qbpm = self.qbpm[~self.pump_status]
        pon_images = self.images[self.pump_status]
        pon_qbpm = self.qbpm[self.pump_status]

        if poff_images.size > 0:
            data["poff"] = poff_images
            data["poff_qbpm"] = poff_qbpm
        if pon_images.size > 0:
            data["pon"] = pon_images
            data["pon_qbpm"] = pon_qbpm

        return data

def get_hdf5_images(file: str, config: ExpConfig) -> npt.NDArray:
    """get images form hdf5"""
    with h5py.File(file, "r") as hf:
        if "detector" not in hf:
            raise KeyError("Key 'detector' not found in the HDF5 file")

        return np.asarray(
            hf[f'detector/{config.param.hutch.value}/{config.param.detector.value}/image/block0_values'],
            dtype=np.float32
        )


if __name__ == "__main__":
    from src.utils.file_util import get_run_scan_directory
    import time

    config: ExpConfig = load_config()
    load_dir: str = config.path.load_dir
    file: str = get_run_scan_directory(load_dir, 146, 1, 40)

    start = time.time()
    # loader = HDF5FileLoader(file)
    images = get_hdf5_images(file, config)
    print(time.time() - start, "sec")

    print(images.shape, "images.shape")


===== .\src\processor\saver.py =====
import os
from abc import ABC, abstractmethod

import numpy.typing as npt
import numpy as np
from scipy.io import savemat
import tifffile

from src.config.config import load_config


class SaverStrategy(ABC):
    @abstractmethod
    def save(self, file_base_name: str, data_dict: dict[str, npt.NDArray], comment: str = ""):
        pass

    @property
    @abstractmethod
    def file(self) -> str:
        pass

    @property
    @abstractmethod
    def file_type(self) -> str:
        pass


class MatSaverStrategy(SaverStrategy):
    def save(self, file_base_name: str, data_dict: dict[str, npt.NDArray], comment: str = ""):
        comment = "_" + comment if comment else ""
        config = load_config()
        mat_dir = config.path.mat_dir

        for key, val in data_dict.items():
            if val.ndim == 3:
                mat_format_images = val.swapaxes(0, 2).swapaxes(0, 1) # TEMP

                mat_file = os.path.join(mat_dir, f"{file_base_name}_{key}{comment}.mat")

                savemat(mat_file, {"data": mat_format_images})
        self._file_name = mat_file

    @property
    def file(self) -> str:
        return self._file_name

    @property
    def file_type(self) -> str:
        return "mat"


class NpzSaverStrategy(SaverStrategy):
    def save(self, file_base_name: str, data_dict: dict[str, npt.NDArray], comment: str = ""):
        comment = "_" + comment if comment else ""
        config = load_config()
        npz_dir = config.path.npz_dir
        npz_file = os.path.join(npz_dir, file_base_name + comment + ".npz")

        np.savez(npz_file, **data_dict)
        self._file_name = npz_file

    @property
    def file(self) -> str:
        return self._file_name

    @property
    def file_type(self) -> str:
        return "npz"


class TifSaverStrategy(SaverStrategy):
    def save(self, file_base_name: str, data_dict: dict[str, npt.NDArray], comment: str = ""):
        comment = "_" + comment if comment else ""
        config = load_config()
        tif_dir = config.path.tif_dir

        for key, val in data_dict.items():
            if val.ndim == 3:

                tif_file = os.path.join(tif_dir, f"{file_base_name}_{key}{comment}.tif")
                tifffile.imwrite(tif_file, val.astype(np.float32))

        self._file_name = tif_file

    @property
    def file(self) -> str:
        return self._file_name

    @property
    def file_type(self) -> str:
        return "tif"


class SaverFactory:
    """Get SaverStrategy"""

    @staticmethod
    def get_saver(file_type) -> SaverStrategy:
        if file_type == 'mat':
            return MatSaverStrategy()
        elif file_type == 'npz':
            return NpzSaverStrategy()
        elif file_type == 'tif':
            return TifSaverStrategy()
        else:
            raise ValueError(f"Unsupported file type: {file_type}")


===== .\src\utils\file_util.py =====
import os
import json
from typing import Optional

from src.config.config import load_config

from roi_rectangle import RoiRectangle
import numpy as np
import numpy.typing as npt
import scipy.io


def get_file_list(mother: str = ".") -> list[str]:
    """
    Get a list of files in the specified directory or the current directory if no directory is specified.

    Args:
        mother (str, optional): The directory path to search for files.
            Defaults to None, which represents the current directory.

    Returns:
        list: A list of filenames in the specified directory.
    """

    files = [file for file in os.listdir(mother) if os.path.isfile(os.path.join(mother, file))]
    return files


def get_folder_list(mother: str = ".") -> list[str]:
    """
    Get a list of folders (directories) in the specified directory or the current directory if no directory is specified.

    Args:
        mother (str, optional): The directory path to search for folders.
            Defaults to None, which represents the current directory.

    Returns:
        list: A list of folder names in the specified directory.
    """
    folders = [folder for folder in os.listdir(mother) if os.path.isdir(os.path.join(mother, folder))]
    return folders


def create_idx_path(mother: str, suffix: str = "") -> str:
    folders = get_folder_list(mother)
    idxes = []
    for folder in folders:
        try:
            idx = int(folder.split("=")[1].split("_")[0])
            idxes.append(idx)
        except (ValueError, IndexError):
            pass
    if idxes:
        idx = max(idxes) + 1
    else:
        idx = 0
    folder_name = f"idx={idx}_{suffix}"
    values_path = os.path.join(mother, folder_name)
    os.makedirs(values_path, exist_ok=True)
    return values_path


def get_run_scan_directory(mother: str, run: int, scan: Optional[int] = None, file_num: Optional[int] = None) -> str:
    """
    Generate the directory for a given run and scan number, optionally with a file number.

    Parameters:
        mother (str): The base directory or path where the path will be generated.
        run (int): The run number for which the path will be generated.
        scan (int, optional): The scan number for which the path will be generated.
            If not provided, only the run directory path will be returned.
        file_num (int, optional): The file number for which the path will be generated.
            If provided, both run and scan directories will be included in the path.

    Returns:
        str: The path representing the specified run, scan, and file number (if applicable).
    """

    if scan is None and file_num is None:
        return os.path.join(mother, f"run={run:0>3}")
    if scan is not None and file_num is None:
        return os.path.join(mother, f"run={run:0>3}", f"scan={scan:0>3}")
    if scan is not None and file_num is not None:
        return os.path.join(mother, f"run={run:0>3}", f"scan={scan:0>3}", f"p{file_num:0>4}.h5")


def create_run_scan_directory(dir: str, run: int, scan: int) -> str:
    """
    Create a nested directory structure for the given run and scan numbers.

    Parameters:
        dir (str): The base directory where the nested structure will be created.
        run (int): The run number for which the directory will be created.
        scan (int): The scan number for which the directory will be created.

    Returns:
        str: The path of the created nested directory.
    """

    os.makedirs(dir, exist_ok=True)
    path = os.path.join(dir, f'run={run:0>3d}')
    os.makedirs(path, exist_ok=True)
    path = os.path.join(path, f'scan={scan:0>3d}')
    os.makedirs(path, exist_ok=True)
    return path


def format_run_scan_filename(run: int, scan: Optional[int] = None, file_num: Optional[int] = None) -> str:
    """
    Generate a formatted file name based on the provided run, scan, and file number.

    Parameters:
        run (int): The run number to be included in the file name.
        scan (int, optional): The scan number to be included in the file name.
            If not provided, only the run number will be included.
        file_num (int, optional): The file number to be included in the file name.
            If provided, both run and scan numbers will be included.

    Returns:
        str: The formatted file name containing run, scan, and file numbers (if applicable) separated by underscores.
    """

    if scan is None and file_num is None:
        return f"run={run:0>3}"
    if scan is not None and file_num is None:
        return "_".join([f"run={run:0>3}", f"scan={scan:0>3}"])
    if scan is not None and file_num is not None:
        return "_".join([f"run={run:0>3}", f"scan={scan:0>3}", f"p{file_num:0>4}"])


def get_roi_list(mother: str) -> Optional[list[RoiRectangle]]:

    file_path = os.path.join(mother, 'ROI_coords.json')
    if os.path.exists(file_path):
        # If paramter file exists, open json file.
        with open(file_path, 'r') as f:
            try:
                roi_rect_list = json.load(f)
            except json.decoder.JSONDecodeError:
                roi_rect_list = None
    else:
        roi_rect_list = None

    if not roi_rect_list:
        roi_rect_list = None

    if isinstance(roi_rect_list, list):
        roi_rect_list = [RoiRectangle(*region) for region in roi_rect_list]

    return roi_rect_list


def get_ooi(mother: str) -> Optional[RoiRectangle]:
    file_path = os.path.join(mother, 'OOI_coords.txt')
    if os.path.exists(file_path):
        # If paramter file exists, open json file.
        with open(file_path, 'r') as f:

            temp = list(map(int, f.readline().split()))
            ooi_rect = RoiRectangle(*temp)

    else:
        ooi_rect = None

    return ooi_rect


def get_sigma_factor(mother: str) -> Optional[float]:

    file_path = os.path.join(mother, 'sigma_factor.txt')
    if os.path.exists(file_path):
        with open(os.path.join(mother, 'sigma_factor.txt'), 'r') as f:
            sig_fac = float(f.read())
    else:
        sig_fac = None

    return sig_fac


def save_roi_list(mother: str, roi_rect_list: list[RoiRectangle]) -> None:
    region_list = [list(region.to_tuple()) for region in roi_rect_list]
    file_name = os.path.join(mother, 'ROI_coords.json')
    with open(file_name, 'w') as f:
        f.write(json.dumps(region_list))


def save_ooi(mother: str, ooi_rect: RoiRectangle) -> None:

    with open(os.path.join(mother, 'OOI_coords.txt'), 'w') as f:
        f.write(f"{ooi_rect.x1} {ooi_rect.y1} {ooi_rect.x2} {ooi_rect.y2}")


def save_sigma_factor(mother: str, sig_fac: float) -> None:
    with open(os.path.join(mother, 'sigma_factor.txt'), 'w') as f:
        f.write(str(sig_fac))


def mat_to_ndarray(run: int, scan: int) -> npt.NDArray:
    config = load_config()
    path = os.path.join(config.path.mat_dir, f'run={run:0>3d}_scan={scan}.mat')
    mat_data = scipy.io.loadmat(path)
    images = mat_data["data"]
    return np.transpose(images, axes=range(images.ndim)[::-1])


===== .\src\utils\math_util.py =====
from typing import Final

import numpy as np
import numpy.typing as npt
from scipy.integrate import quad, dblquad

from src.config.config import load_config


FWHM_COEFFICIENT: Final[float] = 2.35482  # FWHM_COEFFICIENT = 2 * np.sqrt(2 * np.log(2))


def reverse_axis(array: npt.NDArray):
    return np.transpose(array, axes=range(array.ndim)[::-1])


def gaussian(x: npt.NDArray, a: float, mu: float, sig: float) -> npt.NDArray:
    return a * np.exp(-(x - mu) ** 2 / (2 * sig ** 2))


def integrate_FWHM(a: float, mu: float, sig: float) -> float:
    fwhm = FWHM_COEFFICIENT * np.abs(sig)
    result, _ = quad(gaussian, mu - 0.5 * fwhm, mu + 0.5 * fwhm, args=(a, mu, sig))
    return result


def gaussian2d(
    xy, amplitude: float,
    x0: float, y0: float,
    sigma_x: float, sigma_y: float,
    theta: float, offset: float
) -> npt.NDArray:
    """
    Calculate the 2D Gaussian distribution at the given coordinates.

    Parameters:
        xy (tuple of arrays): A tuple containing two arrays 'x' and 'y' representing the 2D coordinates.
        amplitude (float): The amplitude (peak value) of the Gaussian.
        x0 (float): The x-coordinate of the center of the Gaussian.
        y0 (float): The y-coordinate of the center of the Gaussian.
        sigma_x (float): The standard deviation in the x-direction.
        sigma_y (float): The standard deviation in the y-direction.
        theta (float): The rotation angle in radians. The Gaussian will be rotated counterclockwise by this angle.
        offset (float): The constant offset added to the Gaussian distribution.

    Returns:
        numpy.ndarray: A 1D array containing the values of the 2D Gaussian distribution flattened into a 1D array.
    """
    x, y = xy
    a = (np.cos(theta) ** 2) / (2 * sigma_x ** 2) + (np.sin(theta) ** 2) / (2 * sigma_y ** 2)
    b = -(np.sin(2 * theta)) / (4 * sigma_x ** 2) + (np.sin(2 * theta)) / (4 * sigma_y ** 2)
    c = (np.sin(theta) ** 2) / (2 * sigma_x ** 2) + (np.cos(theta) ** 2) / (2 * sigma_y ** 2)
    g = offset + amplitude * np.exp(-(a * ((x - x0) ** 2) + 2 * b * (x - x0) * (y - y0) + c * ((y - y0) ** 2)))
    return g.ravel()


def integrate_fwhm_2d(
    amplitude: float,
    xo: float, yo: float,
    sigma_x: float, sigma_y: float,
    theta: float, offset: float
) -> float:
    """
    Calculate the integral of a 2D Gaussian with an offset over its FWHM
    """
    def integrand(y, x):
        xy = np.meshgrid(x, y)
        return gaussian2d(xy, amplitude, xo, yo, sigma_x, sigma_y, theta, offset)

    # Calculate FWHM in x and y directions
    fwhm_x = FWHM_COEFFICIENT * np.abs(sigma_x)
    fwhm_y = FWHM_COEFFICIENT * np.abs(sigma_y)

    x0 = xo
    y0 = yo
    # Define integration limits based on FWHM
    x_lower = x0 - fwhm_x * 0.5
    x_upper = x0 + fwhm_x * 0.5
    y_lower = y0 - fwhm_y * 0.5
    y_upper = y0 + fwhm_y * 0.5

    # Perform the double integration
    result, _ = dblquad(integrand, y_lower, y_upper, lambda x: x_lower, lambda x: x_upper)
    return result


def pixel_to_del_q(pixels: npt.NDArray) -> npt.NDArray:

    config = load_config()
    del_pixels = pixels - pixels[0]
    del_two_theta = np.arctan2(config.param.dps, config.param.sdd * del_pixels)
    return 4 * np.pi / config.param.wavelength * np.sin(del_two_theta / 2)


def mul_delta_q(pixels: npt.NDArray) -> npt.NDArray:
    config = load_config()
    two_theta = np.arctan2(config.param.dps, config.param.sdd)
    delta_q = (4 * np.pi / config.param.wavelength) * (two_theta)
    return pixels * delta_q


def pixel_to_q(pixels: npt.NDArray) -> npt.NDArray:
    """
    two_theta = arctan(dps * pixels / sdd)
    Q = (4 * pi / wavelength) * sin(two_theta / 2)
      = (4 * pi / wavelength) * two_theta / 2
      = (4 * pi / wavelength) * arctan(dps * pixels / sdd) / 2
      = pixels * (4 * pi / wavelength) * arctan(dps / sdd) / 2
    """
    config = load_config()

    two_theta = np.arctan2(config.param.dps, config.param.sdd * pixels)
    return 4 * np.pi / config.param.wavelength * np.sin(two_theta / 2)


def get_min_max(arr: npt.NDArray) -> tuple[float, float]:

    arr = arr.flatten()
    minimum = maximum = arr[0]
    n = len(arr)
    # If the array length is odd, initialize the variables with the first element
    # Otherwise, compare the first two elements and assign them accordingly
    if n % 2 == 0:
        minimum = min(arr[0], arr[1])
        maximum = max(arr[0], arr[1])
        i = 2
    else:
        i = 1

    # Iterate over pairs of elements, updating the minimum and maximum values
    while i < n - 1:
        if arr[i] < arr[i + 1]:
            minimum = min(minimum, arr[i])
            maximum = max(maximum, arr[i + 1])
        else:
            minimum = min(minimum, arr[i + 1])
            maximum = max(maximum, arr[i])
        i += 2

    return minimum, maximum


def chunck(arr: list, size: int) -> list:
    """divide the list into chunck"""
    return [arr[i:i + size] for i in range(0, len(arr), size)]


def get_most_common_element(arr: npt.NDArray) -> int:
    """
    Get the most common element in a NumPy array along with its count.

    Parameters:
    arr (np.ndarray): Input NumPy array.

    Returns:
    int: The most common element in the array.
    """

    max_val = int(np.max(arr))
    counts, _ = np.histogram(arr, bins=max_val + 1, range=(0, max_val + 1))
    most_common_element = np.argmax(counts)

    return most_common_element


def non_outlier_indices_percentile(
    arr: npt.NDArray,
    lower_percentile: float,
    upper_percentile: float
) -> npt.NDArray[np.bool_]:
    """
    Get the indices of non-outliers in a NumPy array.

    Parameters:
    arr (np.ndarray): Input NumPy array containing data.

    Returns:
    np.ndarray: Boolean array with the same shape as 'arr' where True indicates non-outlier data points.
    """

    # Calculate the first quartile (Q1) and third quartile (Q3)
    q1 = np.percentile(arr, lower_percentile)
    q3 = np.percentile(arr, upper_percentile)

    # Calculate the Interquartile Range (IQR)
    iqr = q3 - q1

    # Define the lower and upper bounds for outliers
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr

    # Create a boolean array to identify non-outliers
    conditions = np.logical_and(arr > lower_bound, arr < upper_bound)

    return conditions